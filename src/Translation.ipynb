{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install tqdm\n",
        "!pip install rouge"
      ],
      "metadata": {
        "id": "qrP9w36Ipugk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "_vD-OhqAoBeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "M_wFIqU2pVdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = load_dataset('GEM/xmediasum', split='validation[:10%]')\n",
        "actual_zh = []\n",
        "actual_de = []\n",
        "for value in val_data:\n",
        "  actual_zh.append(value['summary_zh'])\n",
        "  actual_de.append(value['summary_de'])\n",
        "\n",
        "print(len(actual_zh))\n",
        "print(len(actual_de))"
      ],
      "metadata": {
        "id": "5Txir0ADpT5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read output of Fine Tuned Model ('Generated Text')"
      ],
      "metadata": {
        "id": "UsVkrjJCp-Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_data = pd.read_csv('/content/Abstractive_0_predictions_t5_small_ex_final.csv')\n",
        "summaries = finetune_data[['Generated Text']]\n",
        "generated_summaries = summaries.values.tolist()\n",
        "finetune_summaries = []\n",
        "for sum in generated_summaries:\n",
        "  finetune_summaries.append(sum[0])"
      ],
      "metadata": {
        "id": "F-IJbpKjp920"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read output of Few Shot Model"
      ],
      "metadata": {
        "id": "3ETSNsJDrS11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/few_shot_extractive_r_0.5_train_10_val_10.txt') as f:\n",
        "  few_shot_summaries = f.readlines()"
      ],
      "metadata": {
        "id": "8mkiPlkyrRdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "6jSGk94Bsp8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "1onPXh2csmaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
        "model = model.to(device)\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
      ],
      "metadata": {
        "id": "rKxyNAquscLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(summary, target_language):\n",
        "  tokenizer.src_lang = \"en\"\n",
        "  encoded_hi = tokenizer(summary, return_tensors=\"pt\").to(device)\n",
        "  generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(target_language)).to(device)\n",
        "  translated_summary = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "  return translated_summary[0]"
      ],
      "metadata": {
        "id": "phWdgkSZr4cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translations(arr, target_language):\n",
        "  translated = []\n",
        "  for i in tqdm_notebook(range(len(arr)), desc='Completed'):\n",
        "    translated.append(translate(arr[i], target_language))\n",
        "  return translated"
      ],
      "metadata": {
        "id": "E7EsHUN9tCwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_zh = get_translations(finetune_summaries, 'zh')\n",
        "finetune_de = get_translations(finetune_summaries, 'de')\n",
        "fewshot_zh = get_translations(finetune_summaries, 'zh')\n",
        "fewshot_de = get_translations(finetune_summaries, 'de')"
      ],
      "metadata": {
        "id": "bBgDTlTEtvxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_translation_file(arr, fname):\n",
        "  file = open(fname, 'w')\n",
        "  for v in arr:\n",
        "      file.write(v.encode('ascii', 'ignore').decode('ascii'))\n",
        "      file.write('\\n')\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "XQjM1-9QUeMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_translation_file(finetune_zh, 'Finetune_Zh')\n",
        "create_translation_file(finetune_de, 'Finetune_De')\n",
        "create_translation_file(fewshot_zh, 'Fewshot_Zh')\n",
        "create_translation_file(fewshot_de, 'Fewshot_De')"
      ],
      "metadata": {
        "id": "pyIWHWRpU5BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bleu_response(actual, translated):\n",
        "  bleu_score = 0\n",
        "  for i in range(len(actual)):\n",
        "    reference = []\n",
        "    reference.append(actual[i])\n",
        "    rtokens = [nltk.word_tokenize(ref) for ref in reference]\n",
        "    ttokens = nltk.word_tokenize(translated[i][0])\n",
        "    # print(nltk.translate.bleu_score.sentence_bleu(rtokens, ttokens))\n",
        "    bleu_score += nltk.translate.bleu_score.sentence_bleu(rtokens, ttokens)\n",
        "  \n",
        "  return bleu_score/len(actual)"
      ],
      "metadata": {
        "id": "SuUCu8IamnXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ycHgs8Ln1gKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Finetune Zh : {get_bleu_response(actual_zh, finetune_zh)}')\n",
        "print(f'Finetune De : {get_bleu_response(actual_de, finetune_de)}')\n",
        "print(f'Few Shot Zh : {get_bleu_response(actual_zh, fewshot_zh)}')\n",
        "print(f'Few Shot De : {get_bleu_response(actual_de, fewshot_de)}')"
      ],
      "metadata": {
        "id": "EtKh0QdGzVXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}